{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a deep learning model the clones the driving behavior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this project I have worked on a deep learning model based on LeNet architecture by Yan LeCun to clone the driving behavior of a car.**\n",
    "\n",
    "\n",
    "I have tested this model on Udacity simulator for a complete lap\n",
    "\n",
    "\n",
    "In the following I will be explaining each part of the project ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Pipeline Stepts:\n",
    "\n",
    "**Step 1:** Load The data\n",
    "\n",
    "**Step 2:** Split data into trainig and validation sets\n",
    "    \n",
    "**Step 3:** Define a generator function to be used through training\n",
    "\n",
    "**Step 4:** Use the defined generator in step 3 for training set and validation set\n",
    "    \n",
    "**Step 5:** Using keras, build a regression model based on LeNet architecture to predict the steering angle\n",
    "    \n",
    "In the next, I will be giving some details related to each steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment:\n",
    "* AWS carnd Instance\n",
    "* Python 3.6.4\n",
    "* Anaconda 4.4.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load The data\n",
    "\n",
    "Here I am using the python csv library to read the csv file generated from the simulator.\n",
    "This file contains paths to three camera images captured through the training, steering angle, throttle, break, and vehicle speed.\n",
    "\n",
    "We will ignore the throttle, break, and speed measurements.\n",
    "\n",
    "We will use the images as the feature set and the steering angle as the label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "with open('data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Split data into trainig and validation sets\n",
    "\n",
    "Here I am splitting the data into two set, training set which is 80% of the data, and validation set which is 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8447\n",
      "2112\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(len(train_samples))\n",
    "print(len(validation_samples))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Define a generator function to be used through training and validation\n",
    "\n",
    "Generator is a powerful tool in python. It is used to pull pieces of the data instead of loading it at once in memory. Here in our case, we will be using generators for pre-processing data, surey pieces of them, processing them on the fly only when we need.\n",
    "\n",
    "Here is the steps implemented inside our generator function:\n",
    " * First of all, all the steps are encapsulated in a while(1) loop in roder to prevent the generator from termination. We need the generator alive as long as the training is alive\n",
    " * Everytime, we shuffle the data. **Why** ? *In order not to make the model biased by the order of images*\n",
    " * Then we get a *slice* off our data based on the batch size defined. Here we get a batch of the training samples\n",
    " * For each batch, we get the center, left, and right camera images.\n",
    " * Also, we get the steering angle.\n",
    " * Here, I am usign a correction factor for the steering angles caputured from right and left cameras. The steering angle of the left should be less than the steering angle of the center. The steering angle of the right should be more that the steering angle of the center. Hence, I used this correction factor to compensate this difference.\n",
    " * I made a data augmentation for each image and steering angle by flipping the reading.\n",
    " * Then convert our list images and steeting angles into NumPy lists as this is the type expected by Keras.\n",
    " * Finally, before yielding (return for geterators) the feature and label sets, we shuffle.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            steering_angles = []\n",
    "            correction = 0.1889\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                # Reading center image and steering angle\n",
    "                name = \"data/IMG/\"+batch_sample[0].split('/')[-1]\n",
    "                center_image = cv2.imread(name)\n",
    "                if center_image is not None:\n",
    "                    center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "                    images.append(center_image)\n",
    "                    \n",
    "                    center_angle = float(batch_sample[3])\n",
    "                    steering_angles.append(center_angle)\n",
    "                    \n",
    "                    \n",
    "                    # Making data augmentation - Flippiong readings\n",
    "                    center_image = np.fliplr(center_image)\n",
    "                    images.append(center_image)\n",
    "                    \n",
    "                    center_angle = - center_angle\n",
    "                    steering_angles.append(center_angle)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                else :\n",
    "                    print(\"Center Image \" +  name + \" is NONE\")\n",
    "                    \n",
    "\n",
    "                    \n",
    "                # Reading left image and steering angle\n",
    "                name = \"data/IMG/\"+batch_sample[1].split('/')[-1]\n",
    "                left_image = cv2.imread(name)\n",
    "                if left_image is not None:\n",
    "                    left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB)\n",
    "                    images.append(left_image)\n",
    "                    \n",
    "                    center_angle = float(batch_sample[3])\n",
    "                    left_angle = center_angle + correction\n",
    "                    steering_angles.append(left_angle)\n",
    "                    \n",
    "                    \n",
    "                    # Making data augmentation - Flippiong readings\n",
    "                    left_image = np.fliplr(left_image)\n",
    "                    images.append(left_image)\n",
    "                    \n",
    "                    left_angle = - left_angle\n",
    "                    steering_angles.append(left_angle)\n",
    "                \n",
    "                else :\n",
    "                    print(\"Left Image \" +  name + \" is NONE\")                 \n",
    "                \n",
    "                \n",
    "                # Reading right image and steering angle\n",
    "                name = \"data/IMG/\"+batch_sample[2].split('/')[-1]\n",
    "                right_image = cv2.imread(name)\n",
    "                if right_image is not None:\n",
    "                    right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB)\n",
    "                    images.append(right_image)\n",
    "                    \n",
    "                    center_angle = float(batch_sample[3])\n",
    "                    right_angle = center_angle - correction\n",
    "                    steering_angles.append(right_angle)\n",
    "                    \n",
    "                    \n",
    "                    # Making data augmentation - Flippiong readings\n",
    "                    right_image = np.fliplr(right_image)\n",
    "                    images.append(right_image)\n",
    "                    \n",
    "                    right_angle = - right_angle\n",
    "                    steering_angles.append(right_angle)\n",
    "                \n",
    "                else :\n",
    "                    print(\"Right Image \" +  name + \" is NONE\")  \n",
    "                    \n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(steering_angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Use the defined generator in step 3 for training set and validation set\n",
    "\n",
    "Here I feed for the generator both the training and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image augmentation function which I did use.\n",
    "The function flipps the image then changes its brightness.\n",
    "\n",
    "Flipping is done using np.fliplr which flips the imges horizonally (like a mirror)\n",
    "Changing the brightness is done by converting the image from RGB to HSV  (Hue, Saturation, Value) space, in which the value is responsible on the brightness.\n",
    "The v value is changes then the image is converted back to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image):\n",
    "    flipped_image = np.fliplr(image)\n",
    "    hsv_image = cv2.cvtColor(flipped_image, cv2.COLOR_RGB2HSV) #convert it to hsv\n",
    "\n",
    "    h, s, v = cv2.split(hsv_image)\n",
    "    v += 100\n",
    "    final_hsv_image = cv2.merge((h, s, v))\n",
    "\n",
    "    aug_image = cv2.cvtColor(final_hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return aug_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Using keras, build a regression model based on LeNet architecture to predict the steering angle\n",
    "\n",
    "Here, I made use of the LeNet Architecture by Yan LeCun that is shown below ** with adding dropout for the fully connected layers AND modifying the output layer to be of output 1 as it is regression problem not classification**.\n",
    "\n",
    "![LeNet Architecture](lenet.png)\n",
    "\n",
    "Here I will explain the model input, a brief at each layer mentioning the dimenions of each one\n",
    "\n",
    "### Input\n",
    "At the begging of the model, I added two pre-processing layers to the images:\n",
    "1) Normalization and Mean-centering \"scaling to have a zero-mean images\" \n",
    "2) Cropping:\n",
    "\n",
    "1) **Lambda Layer:** This layer normalizes and scales the images. The lambda parallizes the processing. This layer takes each pixel of the image and applies to it:\n",
    " * Normalization: `pixel_normalized = pixel / 255`\n",
    " * Mean-centering: `pixel_mean_centered = pixel_normalized - 0.5`\n",
    "\n",
    "> The input to the lambda layer is the image captured from the simulator at training which is 160 pixels high, 320 pixels wide, and 3 channels (R, G, B) \n",
    " \n",
    "2) **Cropping Layer:** As the top scene of the image captures hills, trees, sky, and other elements tha may distract our model more than help. Alos, the very bottom of the image shows the hood of the car. So, it makes sense to crop these porions of the image. I am cropping 70 pixels from the top, 20 pixels from the bottom, and no cropping from left and right.\n",
    "\n",
    "> The output of this layer should be an image of 70 pixels high, 320 pixels wide, and 3 channels\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 66 h x 316 w x 6.\n",
    "\n",
    "**Activation.** ELU \"Exponential Linear Unit\" activation function\n",
    "\n",
    "**Pooling.** The output shape should be 33x158x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 29x154x16.\n",
    "\n",
    "**Activation:** ELU activation function\n",
    "\n",
    "**Pooling:** The output shape should be 15x77x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The output should be 18480\n",
    "\n",
    "**Layer 3: Dense \"Fully Connected\" Layer.** This should have 120 outputs.\n",
    "\n",
    "**Activation:** ELU activation function\n",
    "\n",
    "**Dropout:** A dropout is used in order to pretent our model to memorize the training set. We get consensus opinion by averaging the activations (I used a dropout of 0.5).\n",
    "\n",
    "**Layer 4: Dense \"Fully Connected\" Layer.** This should have 84 outputs.\n",
    "\n",
    "**Activation:** ELU activation function.\n",
    "\n",
    "**Dropout:** A dropout is used in order to pretent our model to memorize the training set. We get consensus opinion by averaging the activations (I used a dropout of 0.5)\n",
    "\n",
    "**Layer 5: Dense \"Fully Connected\" Layer (Output -> Regression).** This should have 1 output of predicting the steering angle\n",
    "\n",
    "### Output\n",
    "Steering angle prediction\n",
    "\n",
    "\n",
    "> I used ELU activation function rather than ReLU as ELU function takes care of the Vanishing Gradient Problem. \n",
    "\n",
    "> Here I have used the mean square error loss function as it is the most convenient for regression problems. The yields a derivative cose function (convex).\n",
    "\n",
    "> I used ADAM optimizer as it maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems). [source](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning)\n",
    "\n",
    "At the end, I saved the model to test it on the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "8256/8447 [============================>.] - ETA: 0s - loss: 0.4918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8448/8447 [==============================] - 22s - loss: 0.4844 - val_loss: 0.0698\n",
      "Epoch 2/7\n",
      "8448/8447 [==============================] - 20s - loss: 0.0993 - val_loss: 0.0840\n",
      "Epoch 3/7\n",
      "8448/8447 [==============================] - 20s - loss: 0.0805 - val_loss: 0.0683\n",
      "Epoch 4/7\n",
      "8448/8447 [==============================] - 20s - loss: 0.0786 - val_loss: 0.0766\n",
      "Epoch 5/7\n",
      "8448/8447 [==============================] - 20s - loss: 0.0811 - val_loss: 0.0646\n",
      "Epoch 6/7\n",
      "8634/8447 [==============================] - 21s - loss: 0.0748 - val_loss: 0.0578\n",
      "Epoch 7/7\n",
      "8448/8447 [==============================] - 20s - loss: 0.0681 - val_loss: 0.0697\n"
     ]
    }
   ],
   "source": [
    "# Import Keras packes needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Cropping2D, Lambda\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "#Preparing the input by adding lambda and cropping layers\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,20), (0,0))))\n",
    "\n",
    "# Layer 1: Convuluation 5x5. Output is 66 h x 316 w x 6\n",
    "model.add(Convolution2D(6, 5, 5, border_mode='valid'))\n",
    "# MaxPooling. output is 33x158x6\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "# Layer 1: Convuluation 5x5. Output is 29x154x16.\n",
    "model.add(Convolution2D(16, 5, 5, border_mode='valid'))\n",
    "# MaxPooling. output is 15x77x16\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "# Flatten the convoltion for fully connected layer (Dense)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer. Output is 120\n",
    "model.add(Dense(120))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer. Output is 84\n",
    "model.add(Dense(84))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output \n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model using mean square error  loss function and adam optimizer\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "# Use fit_generator to train and validate the model using the generators defined above.\n",
    "model.fit_generator(train_generator,\n",
    "                    samples_per_epoch=len(train_samples),\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples),\n",
    "                    nb_epoch=7,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save the model for testing it on the simulator\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
